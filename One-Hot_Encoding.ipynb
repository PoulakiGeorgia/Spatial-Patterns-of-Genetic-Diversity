{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The microsatellite dataset is transformated into a binary format applying one hot encoding, while also incorporating infromation on missing values and heterozygosity for each locus."
      ],
      "metadata": {
        "id": "rMe1Mg8Xbo6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data pre-processing:"
      ],
      "metadata": {
        "id": "TV2RusK1b1O_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iT42NWmbcQr"
      },
      "outputs": [],
      "source": [
        "# Load libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "microsat_data = pd.read_excel('pine_ML.xlsx')\n",
        "\n",
        "microsat_data.head()"
      ],
      "metadata": {
        "id": "tMpJKTeEcZ5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store location data\n",
        "location_data = microsat_data['Location']\n",
        "\n",
        "#Remove 'Plant' and 'Location' columns\n",
        "microsat_data = microsat_data.drop('Plant', axis= 1)\n",
        "microsat_data = microsat_data.drop('Location', axis = 1)\n",
        "\n",
        "microsat_data"
      ],
      "metadata": {
        "id": "usD_zZoNcjeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make a list with the column names\n",
        "locus_columns = list(microsat_data.columns)\n",
        "\n",
        "print(locus_columns)"
      ],
      "metadata": {
        "id": "LO-Kz_NBckNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of allele pairs by grouping every two alleles\n",
        "loci_pairs = [(locus_columns[i], locus_columns[i+1]) for i in range(0, len(locus_columns), 2)]\n",
        "\n",
        "print(loci_pairs)"
      ],
      "metadata": {
        "id": "lpNYNca_cmvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find unique repeat lengths for each locus\n",
        "\n",
        "unique_lengths_dict = {}  # Make a dictionary to store unique lengths for each locus\n",
        "\n",
        "for allele1, allele2 in loci_pairs:\n",
        "    combined_unique_lengths = pd.concat([microsat_data[allele1], microsat_data[allele2]]).unique()   # Combine the lengths of the two alleles of each locus and keep only the unique lengths.\n",
        "\n",
        "    # Extract the name of the locus\n",
        "    locus_name = allele1.split('_')[0]\n",
        "\n",
        "    unique_lengths_dict[locus_name] =list(combined_unique_lengths)\n",
        "\n",
        "# Print the dictionary with unique lengths for each locus\n",
        "for locus_name, unique_lengths in unique_lengths_dict.items():\n",
        "    print(f\"Locus: {locus_name}, Unique Lengths: {unique_lengths}\")"
      ],
      "metadata": {
        "id": "lW0X5GtQcqZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extra columns representing Heterozygosity and Missing Values in a binary format are included."
      ],
      "metadata": {
        "id": "eUQm-EgFjeFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column for heterozygosity.\n",
        "\n",
        "for allele1, allele2 in loci_pairs:\n",
        "    # Extract the locus name\n",
        "    locus_name = allele1.split('_')[0]\n",
        "\n",
        "    # Create a new column for heterozygosity (e.g., 'KU85_het')\n",
        "    het_column = f\"{locus_name}_het\"\n",
        "\n",
        "    # Compare the two alleles for each individual\n",
        "    # If alleles are different (heterozygous), assign 1; otherwise, assign 0 (homozygous)\n",
        "    microsat_data[het_column] = (microsat_data[allele1] != microsat_data[allele2]).astype(int)\n",
        "\n",
        "# Check the modified dataframe\n",
        "print(microsat_data.head())"
      ],
      "metadata": {
        "id": "Aju2xjLLctUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing values indicator - Create two new columns (missing_a, missing_b)\n",
        "\n",
        "for locus_name in unique_lengths_dict.keys():  # Use unique lengths dictionary to get locus names\n",
        "\n",
        "    # Check missing values for allele 'a'\n",
        "    microsat_data[f\"{locus_name}_a_missing\"] = microsat_data[f\"{locus_name}_a\"].isna().astype(int)\n",
        "\n",
        "    # Check missing values for allele 'b'\n",
        "    microsat_data[f\"{locus_name}_b_missing\"] = microsat_data[f\"{locus_name}_b\"].isna().astype(int)\n",
        "\n",
        "print(microsat_data)"
      ],
      "metadata": {
        "id": "WUfe1p_fcwNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One Hot Encoding:"
      ],
      "metadata": {
        "id": "DV8Uc_Pvi-SR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list to store the one-hot encoded columns as individual DataFrames\n",
        "encoded_columns = []\n",
        "\n",
        "# Loop over each locus and its unique lengths\n",
        "for locus_name, unique_lengths in unique_lengths_dict.items():\n",
        "\n",
        "    # Loop over each unique length (allele size) for this locus\n",
        "    for unique_length in unique_lengths:\n",
        "\n",
        "        # Create column names for one-hot encoding (for both alleles)\n",
        "        col_name_a = f\"{locus_name}_a-{unique_length}\"\n",
        "        col_name_b = f\"{locus_name}_b-{unique_length}\"\n",
        "\n",
        "        # One-hot encode the allele a: 1 if it matches the unique length, else 0\n",
        "        col_a = (microsat_data[f\"{locus_name}_a\"] == unique_length).astype(int).rename(col_name_a)\n",
        "\n",
        "        # One-hot encode the allele b: 1 if it matches the unique length, else 0\n",
        "        col_b = (microsat_data[f\"{locus_name}_b\"] == unique_length).astype(int).rename(col_name_b)\n",
        "\n",
        "        # Append the two columns as individual DataFrames to the list\n",
        "        encoded_columns.append(col_a)\n",
        "        encoded_columns.append(col_b)\n",
        "\n",
        "# Concatenate all columns in the list into a new DataFrame\n",
        "df_onehot = pd.concat(encoded_columns, axis=1)\n",
        "\n",
        "df_onehot"
      ],
      "metadata": {
        "id": "OlWzW9j2ixo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove nan columns from the dataframe\n",
        "\n",
        "# Drop columns where the column name ends with 'nan'\n",
        "df_onehot = df_onehot.drop(df_onehot.filter(regex='nan$').columns, axis=1)\n",
        "\n",
        "# Display the updated dataframe\n",
        "df_onehot"
      ],
      "metadata": {
        "id": "UiduQ90di0ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add heterozygosity columns from microsat_data to the one-hot encoded data\n",
        "df_het = microsat_data.filter(like='_het')  # Heterozygosity columns\n",
        "\n",
        "# Add missing value columns from microsat_data to the one-hot encoded data\n",
        "df_missing = microsat_data.filter(like='_missing')  # Missing values columns\n",
        "\n",
        "# Combine all columns\n",
        "final_df = pd.concat([df_onehot, df_het, df_missing], axis=1)\n",
        "\n",
        "final_df"
      ],
      "metadata": {
        "id": "voaoRSC4i2h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(final_df.size)\n",
        "print(final_df.shape)"
      ],
      "metadata": {
        "id": "3cGBp1Ehi5La"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}